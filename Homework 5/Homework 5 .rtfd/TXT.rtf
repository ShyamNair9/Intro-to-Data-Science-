{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fmodern\fcharset0 Courier;\f1\fnil\fcharset0 HelveticaNeue;\f2\fmodern\fcharset0 Courier-Bold;
\f3\fmodern\fcharset0 Courier-Oblique;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue109;\red0\green0\blue0;\red14\green115\blue192;
\red255\green255\blue255;\red15\green112\blue1;\red245\green245\blue245;\red38\green38\blue38;\red0\green0\blue255;
\red83\green83\blue83;\red169\green14\blue26;\red51\green110\blue109;\red151\green0\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c50196;\cssrgb\c0\c0\c0;\cssrgb\c0\c53333\c80000;
\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c50196\c0;\cssrgb\c96863\c96863\c96863;\cssrgb\c20000\c20000\c20000;\cssrgb\c0\c0\c100000;
\cssrgb\c40000\c40000\c40000;\cssrgb\c72941\c12941\c12941;\cssrgb\c25098\c50196\c50196;\cssrgb\c66667\c13333\c100000;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs52 \cf0 Introduction to Data Science\cf4 \'b6\cf0 \
\pard\pardeftab720\partightenfactor0

\fs44 \cf0 Homework 5\cf4 \'b6\cf0 \
\pard\pardeftab720\partightenfactor0

\f0\b0\fs28 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 Student Name: Solutions\
Student Netid: Solutions\
\pard\pardeftab720\partightenfactor0
\cf0 \cb3 \
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf0 Part 1: Critique this plan\cf4 \'b6\cf0 \
\pard\pardeftab720\partightenfactor0

\b0\fs28 \cf0 1. After a few beers your CIO invited his buddy from Blue Moon consulting to propose a project using data mining to improve the targeting of the new service that you have been a principal in developing. The service has been quite successful so far, being marketed over the last 6 months via your ingenious, and very inexpensive, word-of-mouth campaign. You've already garnered a pretty large customer base without any targeting, and you've been seeing this success as your best stepping stone to bigger and better things in the firm.\
After some reflection, you've decided that your best course of action is to play a key role in ensuring the success of this data mining project as well. You agree with your CIO's statement in a meeting with Blue Moon, that accurate targeting might cost-effectively expand your audience substantially to consumers that word-of-mouth would not reach. You accept that what Blue Moon says about the characteristics of your service is accurate.\
Based on what we have covered in class and in the book, identify the four most serious weaknesses/flaws in this abridged version of Blue Moon's proposal, and suggest how to ameliorate them. Your answer should be 4 bullet points, each comprising 2-4 sentences: 1-2 sentences stating each weakness, and 1-2 sentences suggesting a better alternative. Maximal credit will be given when the 4 points are as independent as possible.\
\pard\pardeftab720\partightenfactor0

\f0 \cf0 \cb5 --- -------------------------------------------------------------------------- ---\
                            Targeted Audience Expansion             \
                      Prepared by Blue Moon Consulting, Inc.\
\
Your problem is to expand the audience of your new service.  We (Blue Moon) have a \
large database of consumers who can be targeted.  We will build a predictive model \
to estimate which of these consumers are the most likely to adopt the product, and\
then target them with the special offer you have designed.\
\
More specifically, we will build a logistic regression (LR) model to predict adop-\
tion of the service by a consumer, based on the data on your current customers of \
this service.  The model will be based on their demographics and their usage of \
the service. We believe that logistic regression is the best choice of method be-\
cause it is a tried-and-true statistical modeling technique, and we can easily \
interpret the coefficients of the model to infer whether the attributes are stat-\
istically significant, and whether they make sense. If they are statistically sig-\
nificant and they do make sense, then we can have confidence that the model will \
be accurate in predicting service uptake. We will apply the model to our large \
database of consumers, and select out those who have not yet subscribed and whom\
the LR model predicts to be the most likely to subscribe.  To these we will send \
the targeted offer. As this is a fixed-profit-per-customer service, this also \
will in effect rank them by expected profit.\
--- -------------------------------------------------------------------------- ---\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 Answer here!\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf0 Part 2: Naive Bayes\cf4 \'b6\cf0 \
\pard\pardeftab720\partightenfactor0

\f0\b0\fs28 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 1. From your reading you know that the naive Bayes classifier works by calculating the conditional probabilities of each feature, {{\NeXTGraphic math.svg \width640 \height640 \noorient
}¬}, occuring with each class {{\NeXTGraphic 1__#$!@%!#__math.svg \width640 \height640 \noorient
}¬} and treating them independently. This results in the probability of a certain class occuring given a set of features, or a piece of evidence, {{\NeXTGraphic 2__#$!@%!#__math.svg \width640 \height640 \noorient
}¬}, as\
\pard\pardeftab720\partightenfactor0
\cf0 {{\NeXTGraphic 3__#$!@%!#__math.svg \width640 \height640 \noorient
}¬}\pard\pardeftab720\partightenfactor0
\cf0 \
The conditional probability of each piece of evidence occuring with a given class is given by\
\pard\pardeftab720\partightenfactor0
\cf0 {{\NeXTGraphic 4__#$!@%!#__math.svg \width640 \height640 \noorient
}¬}\pard\pardeftab720\partightenfactor0
\cf0 \
In the above equation {{\NeXTGraphic 5__#$!@%!#__math.svg \width640 \height640 \noorient
}¬} is the number of documents in a given class that contain feature {{\NeXTGraphic math.svg \width640 \height640 \noorient
}¬} and {{\NeXTGraphic 6__#$!@%!#__math.svg \width640 \height640 \noorient
}¬} is the number of documents that belong to class {{\NeXTGraphic 1__#$!@%!#__math.svg \width640 \height640 \noorient
}¬}.\
A common variation of the above is to use Laplace (sometimes called +1) smoothing. This is done in sklearn by setting 
\f0 \cb5 alpha=1
\f1 \cb1  in the 
\f0 \cb5 BernoulliNB()
\f1 \cb1  function (this is also the default behavior). The result of Laplace smoothing will slightly change the conditional probabilities,\
\pard\pardeftab720\partightenfactor0
\cf0 {{\NeXTGraphic 7__#$!@%!#__math.svg \width640 \height640 \noorient
}¬}\pard\pardeftab720\partightenfactor0
\cf0 \
In no more than 
\b one paragraph
\b0 , describe why this is useful. Try to think of a case when not using Laplace smoothing would result in "bad" models. Try to give an example. Be precise.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 Answer here!\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf0 Part 3: Text features\cf4 \'b6\cf0 \
\pard\pardeftab720\partightenfactor0

\b0\fs28 \cf0 For this part of the assignment, we are going to use a data set of movie ratings from IMDB.com. The data consists of the text of a movie review and a target variable which tells us whether the reviewer had a positive feeling towards the movie (equivalent to rating the movie between 7 and 10) or a negative feeling (rating the movie between 1 and 4). Neutral reactions are not included in the data.\
The data are located in "
\f0 \cb5 data/imdb.csv
\f1 \cb1 ". The first column is the review text; the second is the text label 'P' for positive or 'N' for negative.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 1. Load the data into a pandas 
\f0 \cb5 DataFrame()
\f1 \cb1 .\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f2\b \cf6 \cb7 import
\f0\b0 \cf8  
\f2\b \cf9 pandas
\f0\b0 \cf8  
\f2\b \cf6 as
\f0\b0 \cf8  
\f2\b \cf9 pd
\f0\b0 \cf8 \
data \cf10 =\cf8  pd\cf10 .\cf8 read_csv(\cf11 "data/imdb.csv"\cf8 )\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 2. Code the target variable to be numeric: use the value 
\f0 \cb5 1
\f1 \cb1  to represent 'P' and 
\f0 \cb5 0
\f1 \cb1  to represent 'N'.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0
\cf8 \cb7 data[\cf11 'Class'\cf8 ] \cf10 =\cf8  pd\cf10 .\cf8 Series(data[\cf11 'Class'\cf8 ] \cf10 ==\cf8  \cf11 "P"\cf8 , dtype\cf10 =\cf6 int\cf8 )\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 3. Put all of the text into a data frame called 
\f0 \cb5 X
\f1 \cb1  and the target variable in a data frame called 
\f0 \cb5 Y
\f1 \cb1 . Make a train/test split where you give 75% of the data to training.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f2\b \cf6 \cb7 from
\f0\b0 \cf8  
\f2\b \cf9 sklearn.cross_validation
\f0\b0 \cf8  
\f2\b \cf6 import
\f0\b0 \cf8  train_test_split\
\
X \cf10 =\cf8  data[\cf11 'Text'\cf8 ]\
Y \cf10 =\cf8  data[\cf11 'Class'\cf8 ]\
\
X_train, X_test, Y_train, Y_test \cf10 =\cf8  train_test_split(X, Y, train_size\cf10 =.75\cf8 )\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 4. Create a binary 
\f0 \cb5 CountVectorizer()
\f1 \cb1  and 
\f0 \cb5 TfidfVectorizer()
\f1 \cb1 . Use the original single words as well as bigrams. Also, use an "english" stop word list. Fit these to the training data to extract a vocabulary and then transform both the train and test data.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f2\b \cf6 \cb7 from
\f0\b0 \cf8  
\f2\b \cf9 sklearn.feature_extraction.text
\f0\b0 \cf8  
\f2\b \cf6 import
\f0\b0 \cf8  CountVectorizer\

\f2\b \cf6 from
\f0\b0 \cf8  
\f2\b \cf9 sklearn.feature_extraction.text
\f0\b0 \cf8  
\f2\b \cf6 import
\f0\b0 \cf8  TfidfVectorizer\
\
binary_vectorizer \cf10 =\cf8  CountVectorizer(binary\cf10 =\cf6 True\cf8 )\
binary_vectorizer\cf10 .\cf8 fit(X_train)\
X_train_binary \cf10 =\cf8  binary_vectorizer\cf10 .\cf8 transform(X_train)\
X_test_binary \cf10 =\cf8  binary_vectorizer\cf10 .\cf8 transform(X_test)\
\
tfidf_vectorizer \cf10 =\cf8  TfidfVectorizer()\
tfidf_vectorizer\cf10 .\cf8 fit(X_train)\
X_train_tfidf \cf10 =\cf8  tfidf_vectorizer\cf10 .\cf8 transform(X_train)\
X_test_tfidf \cf10 =\cf8  tfidf_vectorizer\cf10 .\cf8 transform(X_test)\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 4. Create 
\f0 \cb5 LogisticRegression()
\f1 \cb1  and 
\f0 \cb5 BernoulliNB()
\f1 \cb1  models. For all settings, keep the default values. In a single plot, show the AUC curve for both classifiers and both the binary and tfidf feature sets. In the legend, include the area under the ROC curve (AUC). Do not forget to label your axes. Your final plot will be a single window with 4 curves.\
Which model do you think does a better job? Why? Explain in no more than a paragraph.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f3\i \cf12 \cb7 # Run this so your plots show properly
\f0\i0 \cf8 \
\pard\pardeftab720\partightenfactor0

\f2\b \cf6 import
\f0\b0 \cf8  
\f2\b \cf9 matplotlib.pyplot
\f0\b0 \cf8  
\f2\b \cf6 as
\f0\b0 \cf8  
\f2\b \cf9 plt
\f0\b0 \cf8 \
\cf10 %\cf8 matplotlib inline\
plt\cf10 .\cf8 rcParams[\cf11 'figure.figsize'\cf8 ] \cf10 =\cf8  \cf10 12\cf8 , \cf10 12\cf8 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f2\b \cf6 \cb7 from
\f0\b0 \cf8  
\f2\b \cf9 sklearn.linear_model
\f0\b0 \cf8  
\f2\b \cf6 import
\f0\b0 \cf8  LogisticRegression\

\f2\b \cf6 from
\f0\b0 \cf8  
\f2\b \cf9 sklearn.naive_bayes
\f0\b0 \cf8  
\f2\b \cf6 import
\f0\b0 \cf8  BernoulliNB\

\f2\b \cf6 from
\f0\b0 \cf8  
\f2\b \cf9 sklearn
\f0\b0 \cf8  
\f2\b \cf6 import
\f0\b0 \cf8  metrics\
\

\f2\b \cf6 for
\f0\b0 \cf8  model, model_label 
\f2\b \cf13 in
\f0\b0 \cf8  [(LogisticRegression(), \cf11 "LR"\cf8 ), (BernoulliNB(), \cf11 "NB"\cf8 )]:\
    
\f2\b \cf6 for
\f0\b0 \cf8  train, test, method 
\f2\b \cf13 in
\f0\b0 \cf8  [(X_train_binary, X_test_binary, \cf11 'binary'\cf8 ), (X_train_tfidf, X_test_tfidf, \cf11 'tfidf'\cf8 )]:\
        model\cf10 .\cf8 fit(train, Y_train)\
        fpr, tpr, thresholds \cf10 =\cf8  metrics\cf10 .\cf8 roc_curve(Y_test, model\cf10 .\cf8 predict_proba(X_test_binary)[:,\cf10 1\cf8 ])\
        auc \cf10 =\cf8  metrics\cf10 .\cf8 roc_auc_score(Y_test, model\cf10 .\cf8 predict_proba(test)[:, \cf10 1\cf8 ])\
        plt\cf10 .\cf8 plot(fpr, tpr, label\cf10 =\cf8 model_label \cf10 +\cf8  \cf11 " ("\cf8  \cf10 +\cf8  method \cf10 +\cf8  \cf11 ") AUC = "\cf8  \cf10 +\cf8  \cf6 str\cf8 (\cf6 round\cf8 (auc, \cf10 4\cf8 )))\
        plt\cf10 .\cf8 xlabel(\cf11 "FPR"\cf8 )\
        plt\cf10 .\cf8 ylabel(\cf11 "TPR"\cf8 )\
        plt\cf10 .\cf8 title(\cf11 "ROC Curve"\cf8 )\
plt\cf10 .\cf8 legend()\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 Explanation here!\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 5. Use the model from question 4 that you think did the best job and predict the rating of the test data. Find 5 examples the should have been positive, but were incorrectly classified as negative. List the text below and include an explanation as to why you think it may have been incorrectly classified. You can pick any 5. They do not have to be at random.\
\pard\pardeftab720\partightenfactor0

\f0 \cf2 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f2\b \cf6 \cb7 import
\f0\b0 \cf8  
\f2\b \cf9 numpy
\f0\b0 \cf8  
\f2\b \cf6 as
\f0\b0 \cf8  
\f2\b \cf9 np
\f0\b0 \cf8 \
model \cf10 =\cf8  LogisticRegression()\
model\cf10 .\cf8 fit(X_train_binary, Y_train)\
predictions \cf10 =\cf8  model\cf10 .\cf8 predict(X_test_binary)\
\
positive_locations \cf10 =\cf8  np\cf10 .\cf8 where(Y_test \cf10 ==\cf8  \cf10 1\cf8 )[\cf10 0\cf8 ]\
wrong_entries \cf10 =\cf8  np\cf10 .\cf8 where(predictions[positive_locations] \cf10 !=\cf8  Y_test[positive_locations])[\cf10 0\cf8 ]\
\
X_test[wrong_entries]\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 Explanation for the 5 reviews chosen here!}